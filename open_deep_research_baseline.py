# -*- coding: utf-8 -*-
"""open_deep_research_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1waKX_NTgiY-47bYE0cI6qD8Cjn3zjrL6

# Baseline for an Open-Source Deep Research
Reasoning models and some tool usage, although no conditional execution controlled by the agent.
"""

import os
from pathlib import Path
import logging
from typing import Any, Set, Dict, List, Optional
import json
from json_repair import repair_json
import regex as re
import wikipediaapi
from tqdm import tqdm
import pickle

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
import transformers
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_huggingface import HuggingFaceEmbeddings

# ロギングの設定
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

class DeepResearchAssistant:
    def __init__(
        self,
        model_name: str = "deepseek-r1:1.5b",
        translate_model_name: str = "7shi/gemma-2-jpn-translate:2b-instruct-q8_0",  # 翻訳モデルを追加
        max_seq_length: int = 4048,
        embedding_model: str = "hotchpotch/static-embedding-japanese",
        chroma_dir: str = "./chroma_db",  # Milvus URIをChromaディレクトリに変更
        output_dir: str = "./output",
        ollama_base_url: str = "http://localhost:11434"  # OllamaのベースURL
    ):
        self.model_name = model_name
        self.translate_model_name = translate_model_name
        self.max_seq_length = max_seq_length
        self.chroma_dir = chroma_dir  # Milvus URIをChromaディレクトリに変更
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.ollama_base_url = ollama_base_url
        
        # モデルの初期化
        self._init_models(model_name, translate_model_name, embedding_model)
        
    def _init_models(self, model_name: str, translate_model_name: str, embedding_model: str):
        """モデルの初期化"""
        logger.info("Initializing models...")
        
        # 推論モデルの初期化
        self.model = ChatOllama(
            model=model_name,
            base_url=self.ollama_base_url,
            temperature=0.7,
            streaming=True
        )
        
        # 翻訳モデルの初期化
        self.translate_model = ChatOllama(
            model=translate_model_name,
            base_url=self.ollama_base_url,
            temperature=0.0,
            streaming=True
        )
        
        # 埋め込みモデルの初期化
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'}
        )
        
    def ask_model(self, prompt: str, system_prompt: str = "You are a helpful assistant who answers question truthfully to the best of your knowledge.") -> str:
        """モデルに質問を投げる"""
        logger.info(f"Prompt: {prompt[:200]}...")  # 長いプロンプトは省略して表示
        
        # プロンプトテンプレートの作成
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}")
        ])
        
        # チェーンの作成と実行
        chain = prompt_template | self.model | StrOutputParser()
        response = chain.invoke({"input": prompt})
        logger.info(f"Response: {response[:200]}...")  # レスポンスも省略して表示
        return response

    def extract_json(self, response: str) -> Dict:
        """レスポンスからJSONを抽出"""
        try:
            # <think>タグとその内容を削除
            response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)
            response = response.strip()
            
            # JSONを探す
            match = re.search(r"```json\n(.*?)\n```|(\{.*?\})", response, re.DOTALL)
            if not match:
                # 番号付きリストから辞書を作成
                numbered_items = re.findall(r'\d+\.\s*(.*?)(?=\d+\.|$)', response, re.DOTALL)
                if numbered_items:
                    return {"sub_questions": [item.strip() for item in numbered_items]}
                logger.warning(f"No JSON or numbered list found in response: {response}")
                return {}
                
            json_str = (match.group(1) or match.group(2)).strip()
            return json.loads(repair_json(json_str))
        except Exception as e:
            logger.error(f"JSON extraction failed: {e}\nResponse: {response}")
            return {}

    def extract_topic_from_response(self, response_json: Dict) -> str:
        """レスポンスJSONからトピックを抽出"""
        try:
            return response_json.get('topic', '')
        except Exception as e:
            logger.error(f"Topic extraction failed: {e}")
            return ''

    def translate_to_english(self, text: str) -> str:
        """日本語から英語への翻訳"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", "Translate Japanese to English:"),
            ("assistant", "OK"),
            ("human", "{input}")
        ])
        chain = prompt | self.translate_model | StrOutputParser()
        return chain.invoke({"input": text})

    def translate_to_japanese(self, text: str) -> str:
        """英語から日本語への翻訳"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", "Translate English to Japanese:"),
            ("assistant", "OK"),
            ("human", "{input}")
        ])
        chain = prompt | self.translate_model | StrOutputParser()
        return chain.invoke({"input": text})

    def research_topic(self, query: str, page_title: str) -> Dict:
        """トピックの調査を実行"""
        # 質問を英語に翻訳
        logger.info("Translating query to English...")
        query_en = self.translate_to_english(query)
        logger.info(f"Query (EN): {query_en}")
        
        # ページタイトルを英語に翻訳
        logger.info("Translating page title to English...")
        page_title_en = self.translate_to_english(page_title)
        logger.info(f"Page Title (EN): {page_title_en}")
        
        # トピックの特定
        topic_prompt = (
            "What is the topic of the following question? "
            "Respond in JSON format with a single 'topic' field.\n"
            "Example format:\n"
            "{\n"
            '    "topic": "The reason why NHK broadcasts SpongeBob SquarePants"\n'
            "}\n\n"
            f"Question: {query_en}"
        )
        
        response = self.ask_model(topic_prompt)
        response_json = self.extract_json(response)
        topic_en = self.extract_topic_from_response(response_json)
        topic_ja = self.translate_to_japanese(topic_en)
        
        logger.info(f"Topic (EN): {topic_en}")
        logger.info(f"Topic (JA): {topic_ja}")
        
        # サブクエリの生成と分析
        sub_questions = self._generate_sub_questions(query_en, topic_en)
        breakdown = self._analyze_sub_questions(sub_questions, topic_en)
        
        # Wikipedia記事の取得と分析（英語のページタイトルを使用）
        answers_en = self._analyze_with_wikipedia(page_title_en, breakdown)
        
        # 回答を日本語に翻訳
        answers_ja = {}
        for k, v in answers_en.items():
            answers_ja[k] = self.translate_to_japanese(v)
        
        # レポートの生成（日本語）
        self._generate_report(topic_ja, breakdown, answers_ja)
        
        return {
            'topic': topic_ja,
            'breakdown': breakdown,
            'answers': answers_ja
        }

    def _generate_sub_questions(self, query: str, topic: str) -> List[str]:
        """サブクエリの生成"""
        prompt = (
            "Break down the following question into 3-5 sub-questions.\n"
            "Respond in JSON format with a 'sub_questions' array.\n"
            "Example format:\n"
            "{\n"
            '    "sub_questions": [\n'
            '        "What factors influenced NHK\'s decision to broadcast SpongeBob?",\n'
            '        "How does SpongeBob align with NHK\'s programming strategy?",\n'
            '        "What is the target audience for SpongeBob on NHK?"\n'
            "    ]\n"
            "}\n\n"
            f"Question: {query}\n\n"
            "Provide only the JSON response."
        )
        
        response = self.ask_model(prompt)
        json_response = self.extract_json(response)
        
        if not json_response or 'sub_questions' not in json_response:
            logger.warning(f"Failed to get sub_questions from response: {response}")
            return []
            
        return json_response.get('sub_questions', [])

    def _analyze_sub_questions(self, sub_questions: List[str], topic: str) -> Dict:
        """サブクエリの分析"""
        breakdown = {}
        for q in sub_questions:
            prompt = f"""You are researching the following topic. Break down the question into sub-questions.
            
            Topic: {topic}
            Question: {q}"""
            
            response = self.ask_model(prompt)
            response_json = self.extract_json(response)
            sub_sub_questions = list(self._extract_leaves(response_json))
            breakdown[q] = sub_sub_questions
            
        return breakdown

    @staticmethod
    def _extract_leaves(struct: Any) -> Set[Any]:
        """ネストされた構造から末端の値を抽出"""
        values = set()
        
        def add_leaves(struct_: Any) -> None:
            if isinstance(struct_, dict):
                for sub_struct in struct_.values():
                    add_leaves(sub_struct)
            elif isinstance(struct_, list):
                for sub_struct in struct_:
                    add_leaves(sub_struct)
            elif struct_ is not None:
                values.add(struct_)
                
        add_leaves(struct)
        return values

    def _analyze_with_wikipedia(self, page_title: str, breakdown: Dict) -> Dict[str, str]:
        """Wikipediaの記事を使用して質問に回答する"""
        logger.info("Analyzing with Wikipedia...")
        
        # Wikipediaの記事を取得
        wiki_wiki = wikipediaapi.Wikipedia(
            user_agent='DeepResearchBot/1.0',
            language='en'
        )
        page = wiki_wiki.page(page_title)
        
        # テキストを分割
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=200
        )
        docs = text_splitter.create_documents([page.text])
        
        # ベクトルストアの作成（MilvusからChromaに変更）
        vectorstore = Chroma.from_documents(
            documents=docs,
            embedding=self.embeddings,
            persist_directory=self.chroma_dir,  # 永続化ディレクトリを指定
            collection_name=f"wiki_{page_title.lower().replace(' ', '_')}"  # コレクション名を動的に設定
        )
        
        # RAGチェーンの設定（以下は同じ）
        retriever = vectorstore.as_retriever(
            search_type="similarity",  # 検索タイプを明示的に指定
            search_kwargs={"k": 4}  # 取得する文書数を指定
        )
        
        # Hugging Face パイプラインの代わりにChatOllamaを使用
        llm = self.model
        
        # プロンプトテンプレートの設定
        PROMPT_TEMPLATE = """
        You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.
        Use the following pieces of information to provide a concise answer to the question enclosed in $question$ tags.
        If you don't know the answer, just say that you don't know, don't try to make up an answer. Answer in a single short paragraph.
        $context$
        {context}
        $/context$

        $question$
        {question}
        $/question$
        """
        
        prompt = PromptTemplate(
            template=PROMPT_TEMPLATE,
            input_variables=["context", "question"]
        )
        
        # RAGチェーンの構築
        rag_chain = (
            {"context": retriever | self._format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        # 質問への回答を生成
        answers = {}
        total = sum(1 for v in breakdown.values() for _ in (v if v else [None]))
        
        with tqdm(total=total) as pbar:
            for k, v in breakdown.items():
                if not v:
                    logger.info(f"Answering question: {k}")
                    answers[k] = rag_chain.invoke(k).split('</think>')[-1].strip()
                    pbar.update(1)
                else:
                    for q in v:
                        logger.info(f"Answering sub-question: {q}")
                        answers[q] = rag_chain.invoke(q).split('</think>')[-1].strip()
                        pbar.update(1)
        
        return answers

    @staticmethod
    def _format_docs(docs: List[str]) -> str:
        """ドキュメントをフォーマットする"""
        return "\n\n".join(doc.page_content for doc in docs)

    def _generate_report(self, topic: str, breakdown: Dict, answers: Dict) -> None:
        """最終レポートを生成する"""
        logger.info("Generating report...")
        
        report = [f'# {topic}\n\n']
        for k, v in breakdown.items():
            # セクションタイトルを日本語に翻訳
            section_title = self.translate_to_japanese(k)
            report.append(f'## {section_title}\n')
            if not v:
                report.append(answers[k] + '\n\n')
            else:
                for q in v:
                    # サブセクションタイトルを日本語に翻訳
                    subsection_title = self.translate_to_japanese(q)
                    report.append(f'### {subsection_title}\n')
                    report.append(answers[q] + '\n\n')
        
        md = ''.join(report)
        report_path = self.output_dir / 'report.md'
        with open(report_path, 'w', encoding='utf-8') as f:  # エンコーディングを指定
            f.write(md)
        
        # 回答も保存
        answers_path = self.output_dir / 'answers.pkl'
        with open(answers_path, 'wb') as f:
            pickle.dump(answers, f)
        
        logger.info(f"Report generated: {report_path}")

def main():
    # 設定
    query = "なぜNHKはスポンジボブを放送するのか?"
    page_title = "スポンジボブ"
    
    # アシスタントの初期化
    assistant = DeepResearchAssistant()
    
    # 研究の実行
    results = assistant.research_topic(query, page_title)
    logger.info(results)
    
    logger.info("Research completed successfully")

if __name__ == "__main__":
    main()

